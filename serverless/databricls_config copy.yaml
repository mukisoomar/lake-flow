%yaml
# Databricks Environment Configuration

# Serverless SQL Warehouse Configuration
serverless_sql_warehouse:
  name: "serverless-warehouse"
  cluster_size: "2X-Small"
  min_num_clusters: 1
  max_num_clusters: 5
  auto_stop_mins: 30
  enable_serverless_compute: true
  spot_instance_policy: "COST_OPTIMIZED"
  tags:
    - key: "Environment"
      value: "Development"
    - key: "Project"
      value: "LakeFlow"

# Libraries to Install
libraries:
  - pypi:
      package: "pandas==1.3.3"
  - pypi:
      package: "numpy==1.21.2"
  - maven:
      coordinates: "org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1"
  - jar:
      path: "dbfs:/mnt/jars/custom-jar.jar"

# Environment Variables
environment_variables:
  DATABRICKS_TOKEN: "your-databricks-token"
  AWS_ACCESS_KEY_ID: "your-aws-access-key-id"
  AWS_SECRET_ACCESS_KEY: "your-aws-secret-access-key"

# Databricks SQL Configuration
sql:
  catalog: "main"
  schema: "lake_flow_demo"
  permissions:
    - user: "your-username"
      permission: "USE_CATALOG"
    - user: "your-username"
      permission: "CREATE"
    - user: "your-username"
      permission: "USE_SCHEMA"

# Sample Data Configuration
sample_data:
  database: "lake_flow_demo"
  tables:
    - name: "orders"
      path: "dbfs:/mnt/data/orders"
      format: "delta"
    - name: "customers"
      path: "dbfs:/mnt/data/customers"
      format: "delta"

# Auto-Optimization Settings
auto_optimization:
  delta_auto_optimize: true
  delta_auto_compact: true
  delta_enable_change_data_feed: true

# Liquid Clustering (Databricks Runtime 13.3+)
liquid_clustering:
  enabled: true
  clustering_columns: ["region", "product_category"]

# Multi-Hop Architecture
multi_hop:
  bronze_table: "bronze_orders"
  silver_table: "silver_orders"
  gold_table: "gold_orders"

# Logging Configuration for Job Execution
logging:
  log_path: "dbfs:/logs/job_execution"
  log_level: "INFO"
  enable_audit_logs: true
  enable_metrics: true
  metrics_path: "dbfs:/metrics/job_execution"